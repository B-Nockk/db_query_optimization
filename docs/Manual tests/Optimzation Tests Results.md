# Endpoint tests

## Test 1

> â¯ curl -i http://localhost:8080/patients/005ec40b-1da9-440f-9e14-7c7807b23f32

this test was done on a 500 row db table without any optimization strategies (not even indexing) with a pool of 1

# Result data

```python
details = {
    "client_host": "127.0.0.1",
    "user_agent": "curl/8.5.0",
    "path_params": {"patient_id": "005ec40b-1da9-440f-9e14-7c7807b23f32"},
    "request_id": "f1250f8d-8959-4fe8-b9cd-a8f4bbc7b8c0",
    "content_length": 287,
}
is_error = False
is_slow = False
metadata = {
    "method": "GET",
    "path": "/patients/005ec40b-1da9-440f-9e14-7c7807b23f32",
    "status_code": 200,
    "duration_ms": 39.69,
    "duration_seconds": 0.04,
}
optimization_warnings = ["ðŸ¢ HIGH_DB_WAIT: DB work took 38.96ms of total 39.69ms"]
performance = {
    "total_ms": 39.65,
    "app_logic_ms": 39.61,
    "db_session_total_ms": 38.96,
    "sql_execution_total_ms": 3.64,
    "query_count": 1,
    "db_overhead_ms": 35.32,
}

```

---

### Interpretation - Performance Data Interpretation

---

Your results are **very revealing** for a database optimization project! Here's what I see:

### ðŸŽ¯ Key Findings (500 rows, no optimization, pool=1)

```
Total Request Time: 39.69ms
â”œâ”€ App Logic: 39.61ms (99.8%)
â”‚  â””â”€ DB Session: 38.96ms (98.2%)
â”‚     â”œâ”€ SQL Execution: 3.64ms (9.2%)
â”‚     â””â”€ DB Overhead: 35.32ms (89.0%) âš ï¸
â””â”€ Query Count: 1
```

### ðŸ”´ Critical Issue: **89% DB Overhead**

**What this means:**

- Your SQL query only took **3.64ms** to execute
- But the DB session management took **35.32ms** (connection pooling, transaction handling)
- **You're spending 10x more time managing the connection than executing the query!**

### Root Cause Analysis

With `pool_size=1`, you're likely experiencing:

1. **Connection Pool Contention**

   - Single connection means requests queue up
   - Each request waits for the connection to be released
   - High overhead from connection checkout/checkin

2. **Transaction Management Overhead**
   - SQLAlchemy session setup/teardown
   - AUTOCOMMIT handling
   - Connection state management

### ðŸ“Š What to Test Next

Here's your optimization roadmap with expected impact:

#### **Phase 1: Pre-Optimization Baseline (Current)**

```python
# Current: pool_size=1, no indexes
Expected: 35-40ms total, 89% overhead âœ… (You are here)
```

#### **Phase 2: Add Indexing**

```python
# Test: Add index on patient_id column
CREATE INDEX idx_patient_id ON patients(patient_id);

Expected Impact:
- SQL execution: 3.64ms â†’ 1-2ms (faster lookup)
- DB overhead: Still ~35ms (pool=1 bottleneck remains)
- Total: ~37ms (minor improvement)
```

#### **Phase 3: Optimize Connection Pool**

```python
# Test: Increase pool size
pool_size=5, max_overflow=10

Expected Impact:
- DB overhead: 35.32ms â†’ 5-10ms (huge drop!)
- Total: ~8-12ms
- This will have THE BIGGEST impact
```

#### **Phase 4: Scale to 10K rows**

```python
Expected:
- With index + pool: Still ~8-15ms
- Without index: 50-100ms+ (table scan grows)
```

#### **Phase 5: Scale to 100K rows**

```python
Expected:
- With index + pool: ~10-20ms
- Without index: 200-500ms+ (linear scan kills performance)
```

#### **Phase 6: Scale to 1M rows**

```python
Expected:
- With index + pool: ~15-30ms
- Without index: 2-5 seconds (unusable)
```

#### **Phase 7: Scale to 10M rows**

```python
Expected:
- With index + pool + partitioning: ~20-50ms
- Without optimization: 20-50 seconds (catastrophic)
```

### ðŸŽ¯ Optimization Priority Order

Based on your data:

1. **HIGHEST IMPACT: Fix Connection Pool** (89% overhead â†’ ~10%)

   ```python
   pool_size=5  # Up from 1
   max_overflow=10
   pool_pre_ping=True
   ```

2. **HIGH IMPACT: Add Indexes** (3.64ms â†’ 1-2ms now, critical at scale)

   ```python
   CREATE INDEX idx_patient_id ON patients(patient_id);
   ```

3. **MEDIUM IMPACT: Query Optimization**

   - Use `selectinload()` for relationships (prevent N+1)
   - Add query comments (you already have this!)

4. **FUTURE: Partitioning** (only needed at 10M+ rows)

### ðŸ“ˆ Expected Results Timeline

| Phase         | Rows | Pool | Index | Total Time | Overhead | Notes              |
| ------------- | ---- | ---- | ----- | ---------- | -------- | ------------------ |
| **Current**   | 500  | 1    | âŒ    | 39.69ms    | 89%      | Baseline           |
| Index Only    | 500  | 1    | âœ…    | ~37ms      | 88%      | Minor gain         |
| **Pool Only** | 500  | 5    | âŒ    | ~8ms       | 20%      | **HUGE gain**      |
| Both          | 500  | 5    | âœ…    | ~6ms       | 15%      | Optimal            |
| Scaled        | 10K  | 5    | âœ…    | ~8ms       | 20%      | Still fast         |
| Scaled        | 100K | 5    | âœ…    | ~12ms      | 25%      | Acceptable         |
| Scaled        | 1M   | 5    | âœ…    | ~25ms      | 30%      | Good               |
| Scaled        | 10M  | 5    | âœ…    | ~50ms      | 35%      | Needs partitioning |

### ðŸ”¬ Testing Strategy

```python
# 1. Current baseline (DONE)
pool=1, no index, 500 rows â†’ 39.69ms âœ…

# 2. Add index only
pool=1, with index, 500 rows â†’ expect ~37ms

# 3. Pool only (THIS WILL BLOW YOUR MIND)
pool=5, no index, 500 rows â†’ expect ~8ms

# 4. Both optimizations
pool=5, with index, 500 rows â†’ expect ~6ms

# 5. Scale test with optimizations
pool=5, with index, 10K rows â†’ expect ~8ms
pool=5, with index, 100K rows â†’ expect ~12ms
pool=5, with index, 1M rows â†’ expect ~25ms
pool=5, with index, 10M rows â†’ expect ~50ms
```

### ðŸ’¡ Key Insight

**Your bottleneck is NOT the databaseâ€”it's connection management!**

The `db_overhead_ms: 35.32ms` metric is pure gold. It immediately tells you where to focus. At 500 rows with a simple lookup, the SQL is blazing fast (3.64ms). The problem is your pool configuration.

This is a **textbook case** of premature optimization vs. right optimization:

- âŒ Bad: "Let's add indexes first!" (minor impact)
- âœ… Good: "Let's fix the 89% overhead!" (10x improvement)

## Your metrics are doing exactly what they shouldâ€”pointing you to the real problem! ðŸŽ¯

---

## Test 2

**Test 500 row db table with pool size of 10**

> 2026-01-24 14:00:19 [info ]

- host='172.22.48.1'
- port=5432
- name='dbo_dev'
- username='dbo_dev'
- password=SecretStr('\***\*\*\*\*\***')
- pool_size=10
- max_overflow=20
- pool_timeout=30
- pool_recycle=3600
- ssl_mode=None
- ssl_cert_path=None
- ssl_key_path=None
- ssl_ca_path=None
- driver=<DbDriver.ASYNCPG: 'asyncpg'>

### Result Data

```py
details = {
    "client_host": "127.0.0.1",
    "user_agent": "curl/8.5.0",
    "path_params": {"patient_id": "005ec40b-1da9-440f-9e14-7c7807b23f32"},
    "request_id": "b47c2aa9-91b3-4879-bbd0-9aa50b9b44ef",
    "content_length": 287,
}
is_error = False
is_slow = False
metadata = {
    "method": "GET",
    "path": "/patients/005ec40b-1da9-440f-9e14-7c7807b23f32",
    "status_code": 200,
    "duration_ms": 50.35,
    "duration_seconds": 0.05,
}
optimization_warnings = ["ðŸ¢ HIGH_DB_WAIT: DB work took 49.04ms of total 50.35ms"]
performance = {
    "total_ms": 50.29,
    "app_logic_ms": 50.22,
    "db_session_total_ms": 49.04,
    "sql_execution_total_ms": 7.64,
    "query_count": 1,
    "db_overhead_ms": 41.4,
}

db_result = {
    "name": "Patient_34",
    "date_of_birth": "1990-01-01",
    "gender": "Male",
    "contact_info": "555-0200 #34",
    "medical_history": "None",
    "patient_id": "005ec40b-1da9-440f-9e14-7c7807b23f32",
    "patient_code": "9590704914",
    "created_at": "2026-01-22T16:18:18.402116Z",
    "updated_at": "2026-01-22T16:18:18.402532Z",
}

```

---

### Interpretation - Test 2 Performance Data - Interpretation

---

You're absolutely right! The pool size **wasn't the issue** - I made an incorrect assumption. Let me re-analyze with the actual data:

## ðŸ” Real Analysis: Pool Size 10 Test

```python
# Test 1: pool_size=1
db_overhead_ms: 35.32ms (89% of 39.69ms total)
sql_execution_total_ms: 3.64ms

# Test 2: pool_size=10
db_overhead_ms: 41.4ms (82% of 50.35ms total)  # WORSE!
sql_execution_total_ms: 7.64ms
```

**Pool size made it SLOWER, not faster!** This tells us something completely different.

## ðŸŽ¯ The Real Bottleneck: **First Connection Overhead**

Since you're on **localhost** with **only one call**, the overhead is likely from:

1. **Initial connection establishment** (first connection to the pool)
2. **Connection pool initialization**
3. **PostgreSQL session setup** (setting timezone, client encoding, etc.)
4. **asyncpg driver overhead**

### Why This Happens on First Request

```python
# What happens on the FIRST request to a fresh pool:
1. Pool creates initial connections (pool_pre_ping=True checks each)
2. PostgreSQL server authenticates each connection
3. Each connection runs SET commands (timezone, encoding, etc.)
4. Driver performs protocol negotiation

# With pool_size=10:
- More connections to initialize = MORE overhead
- Each connection has setup cost
```

## ðŸ§ª Test This Theory

Run **2-3 requests in a row** and compare:

```bash
# First request (cold start)
curl http://localhost:8080/patients/005ec40b-1da9-440f-9e14-7c7807b23f32

# Second request (warm pool)
curl http://localhost:8080/patients/005ec40b-1da9-440f-9e14-7c7807b23f32

# Third request (warm pool)
curl http://localhost:8080/patients/005ec40b-1da9-440f-9e14-7c7807b23f32
```

**Expected results:**

- Request 1: ~40-50ms (pool initialization)
- Request 2: ~5-10ms (pool is warm!)
- Request 3: ~5-10ms (pool is warm!)

If the second request is **much faster**, that confirms the overhead is from pool initialization.

## ðŸ”§ Reduce First-Request Overhead

### Option 1: Lazy Pool Initialization (SQLAlchemy Default)

Your pool is created at startup but connections are made **on-demand**. Change this:

```python
# In DbManager.__init__, add:
self.engine: AsyncEngine = create_async_engine(
    url=url,
    # ... your existing params ...
    pool_pre_ping=True,
    # NEW: Create connections immediately at startup
    # This moves the overhead to app startup instead of first request
)
```

Then add a warmup method:

```python
# In DbManager class
async def warmup_pool(self) -> None:
    """
    Pre-create pool connections at startup.
    Moves connection overhead from first request to app startup.
    """
    try:
        # Get pool_size connections to force creation
        tasks = []
        for _ in range(self._config["pool_size"]):
            tasks.append(self.engine.connect())

        connections = await asyncio.gather(*tasks)

        # Close them (returns to pool)
        for conn in connections:
            await conn.close()

        self.logger.info(f"âœ“ Warmed up {len(connections)} pool connections")
    except Exception as e:
        self.logger.error(f"Pool warmup failed: {e}")
        raise
```

Then in `main.py` lifespan:

```python
@asynccontextmanager
async def lifespan(app: FastAPI):
    # ... existing code ...
    db_manager = DbManager.from_config(_db_config)
    await db_manager.verify_connection()
    await db_manager.verify_migrations_current()

    # NEW: Warm up the pool
    await db_manager.warmup_pool()  # <-- Add this

    app.state.db_manager = db_manager
    yield
    # ... shutdown code ...
```

### Option 2: Reduce Pool Size for Single-User Testing

For **localhost testing with one user**, a huge pool is overkill:

```bash
# .env for testing
DB_POOL_SIZE="2"  # Just 2 connections
DB_MAX_OVERFLOW="3"
```

This will reduce initialization overhead.

### Option 3: Disable pool_pre_ping for Testing

```python
# In DbManager.from_config or __init__
pool_pre_ping=False,  # Skip health checks on checkout
```

âš ï¸ **Not recommended for production**, but will show if pre-ping is causing the delay.

## ðŸ“Š What You Should Measure Next

Your **real optimization target** should be:

1. **Warm pool performance** (2nd, 3rd, 4th requests)
2. **Effect of adding indexes** on warm requests
3. **Scaling to 10K rows** with warm pool

The 40ms overhead you're seeing is **one-time startup cost**, not representative of real performance.

## ðŸŽ¯ Updated Testing Strategy

```bash
# Phase 1: Measure warm pool (IMPORTANT!)
# Run 5 requests, ignore first one
for i in {1..5}; do
  curl http://localhost:8080/patients/005ec40b-1da9-440f-9e14-7c7807b23f32
done

# Expected: First ~40ms, rest ~5-10ms

# Phase 2: Add index, measure warm pool
CREATE INDEX idx_patient_id ON patients(patient_id);

# Phase 3: Scale to 10K rows
# Run warmup script, then test

# Phase 4: Add N+1 query scenarios
# Test with relationships (e.g., fetch patient + all their records)
```

## ðŸ’¡ Key Insight

Your metrics are **working perfectly**! They revealed:

- âœ… SQL execution is fast (3-7ms)
- âœ… The overhead is in connection management
- âœ… This is **expected** for first-request-after-startup

The "HIGH_DB_WAIT" warning is technically correct, but it's **not a bug** - it's the cost of establishing secure, validated database connections.

## **TL;DR:** Run multiple requests. The first one pays for pool initialization. Subsequent requests should be ~5-10ms total.

---

## Test 3

---
